# sber_test_task

## Contents
1. [Постановка](#постановка)
2. [Решение](#решение)
2. [Запуск](#запуск)

## Постановка
### Подготовка набора данных для задачи Text to Speech (TTS)
#### Дано:
- модель распознавания речи (en) – функция transcribe на вход
принимает путь к файлу .wav, на выходе распознанный текст.
Функция transcribe дает текст со знаками препинания. Пример
вызова функции описан ниже*
- видеофайл в формате mp4
- библиотеки python для обработки звука

#### Разработать скрипт для подготовки датасета:
- на входе: видеофайл в формате mp4
- задача скрипта: разделить аудио в видеофайле на фрагменты
по 3–5 секунд и распознать их текст при помощи модели для
распознавания речи, результат (wav + text) сохранить на диск
- на выходе: набор файлов wav, каждый длиной 3-5 с; текст, соответствующий каждому файлу wav

#### Результат

В данном задании будут оцениваться читаемость кода и выбранная стратегия для нарезки фрагментов.

#### script for transcription
```
# 
import whisper

model = whisper.load_model("base")
result = model.transcribe("audio.mp3")
print(result["text"])

# source code -> https://github.com/openai/whisper
```

## Решение
Был взят следующий алгоритм:
1. Извлекаем аудио из видео с помощью библиотеки `moviepy`, сохраняем в файл `audio.wav`.
2. С помощью библиотеки `thinkdsp` извлекаем аудио дорожку и удаляем созданный `audio.wav`.
3. Делаем *"грубую нарезку"*
    1. Нарезаем дорожку на куски длинной 0.1 с.
    2. Проверяем на каких отрезках, длиною 0.1 с, *"есть звук"*. То есть хотя бы одно значение амплитуды сигнала на отрезке по модулю превышает 0.1. 
    3. Объединяем отрезки на которых *"есть звук"* и полученные куски *"немного расширяем"*. То есть к каждому куску добавляем по сигналу ддлиной 0.1 с, чтобы точно не обрезать начало или конец слова.
4. Делаем нарезку уже полученных отрезков. Так как требуется, чтобы длительность выодных файлов была от 3 до 5 с, то будем делить отрезок, пока каждая его часть будет не больше 2 с.
    1. Получаем *временные значения*, где амплитуда принимает по модулю значение, большее, чем 0.2.
    3. Сделаем мысленную нарезку по временным значениям из предыдущего пункта и отсортируем отрезки по продолжительности.
    4. Пока итоговая нарезка имеет отрезки с продолжительностью > 2 с, будем брать самый большой отрезок из предыдущего пункта и делать еще одну точку нарезки ровно по середине взятого отрезка.
5. Формируем итоговые файлы. Проходимся по каждому отрезку и буде их записывать во *"фрагменты"*.
    1. Если следующий отрезок *"далеко"*, то есть до него есть большой отрезок *"тишины"*, то записываем отрезок и фрагменты *"тишины"*.
    2. Если при добавлении нового отрезка в записываемый файл продолжительность файла будет меньше 5 с, то добавляем его в фрагмент
    3. Если при добавлении нового отрезка в записываемый файл продолжительность файла будет больше 5 с, то в файл записываем фрагмент, а новый отрезок пойдет в следующий фрагмент.
    4. Если вдруг получилось, что отрезок получился больше 5 с, то просто нарезаем его на куски по 5 с.
6. Проходимся по каждому аудио файлы и создаем соответсвтующий текстовый файл, в котором будет записан распознанный текст.

## Запуск

Для запуска потребуются установить следующие библиотеки: `numpy`, `scipy`, `whisper`, `progress`, `thinkdsp` (есть в репозитории).

Чтобы установить библиотеки необходимо ввести следующую команду:
```
pip install -U numpy scipy openai-whisper progress
```

#### Запуск скрипта:
```
python translator.py video_path
```
`video_path` - Путь до видео файла.

В ходе работы скрипта будет создана папка `output/` с аудио файлами вида `audio_{index}.wav` и соответствующие им текстовые файлы `text_{index}.txt`